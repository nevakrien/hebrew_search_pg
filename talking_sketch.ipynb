{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5625248b-b084-4d0f-aaa3-77d5021b7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_model_name=\"dicta-il/dictalm-7b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cff53ab-75a5-4782-92fa-742e1210a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda02289-5aa3-4bb6-be1e-b2a1f6b472a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e50cc2a84d4bab9f8aabad0f4702b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(talk_model_name,trust_remote_code=True)\n",
    "tokenizer=AutoTokenizer.from_pretrained(talk_model_name,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27700da8-f02a-41b9-b6c5-722e9a31e19c",
   "metadata": {},
   "source": [
    "# main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "186574a6-0c62-4568-a3fc-8ea56a8e524d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MegatronGPTForCausalLM(\n",
       "  (megatron_gpt): MegatronGPTModel(\n",
       "    (embed_in): Embedding(56064, 4096)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MegatronGPTLayer(\n",
       "        (input_layernorm): MegatronGPTLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): MegatronGPTLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (self_attention): MegatronGPTAttention(\n",
       "          (rotary_emb): MegatronGPTRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): MegatronGPTMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=4096, out_features=10880, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=10880, out_features=4096, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): MegatronGPTLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=4096, out_features=56064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e11063ba-e50b-4a6c-9c53-448558ccdc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(text, question):\n",
    "    \"\"\"\n",
    "    Generates a prompt for the model to encourage direct quoting from the text in response to a question.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The Hebrew text to be summarized.\n",
    "    - question (str): The user's specific question.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated prompt for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    טקסט: {text}\n",
    "    שאלה: {question}\n",
    "    הנחיות: נא לצטט באופן ישיר מהטקסט כחלק מהתשובה. התשובה צריכה להתייחס במדויק לפרטים המופיעים בטקסט.\n",
    "    תשובה:\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c572592e-8dd1-4ea9-89e5-1f46d1a4a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def generate_summary(text, question,model=model, tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Generates a summary of a given Hebrew text from the Supreme Court decisions database,\n",
    "    focusing on aspects relevant to the user's question.\n",
    "\n",
    "    Args:\n",
    "    - model (transformers.PreTrainedModel): The loaded Hugging Face model.\n",
    "    - tokenizer (transformers.PreTrainedTokenizer): The loaded tokenizer.\n",
    "    - text (str): The Hebrew text to summarize.\n",
    "    - question (str): The user's specific question to focus the summary on.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine the text and the question to provide context to the model\n",
    "    prompt = generate_prompt(text,question)#f\"טקסט: {text}\\nשאלה: {question}\\nתקציר:\"\n",
    "\n",
    "    # Encode the combined prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    # Generate the response\n",
    "    output = model.generate(input_ids, max_new_tokens=150, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decode the generated text\n",
    "    summary = tokenizer.decode(output[0][input_ids.shape[-1]:].cpu(), skip_special_tokens=True)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ecf2a-61cb-4dc1-b412-e6aba7f945e2",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab7bb68a-d833-4335-8049-81f2f5ee66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"כשהייתי בן שש, ראיתי פעם בספר על יער ויראני שכותרתו הייתה 'סיפורים מחיי החיות' תמונה מרהיבה. תיארה את נחש בולע חיה. הנה העתק של הציור. כתוב בספר: 'נחשי הבואה בולעים את טרפם כולו, בלי ללעוס אותו. אחר כך אינם יכולים לנוע והם ישנים לאורך ששת החודשים שלוקח להם לעכל.'\"\n",
    "question=\"מה ראה הנסיך הקטן כשהיה בן שש שהשפיע עליו כל כך?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "652a83de-2804-42d0-be2d-323d69f89bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' : \"נחשים בולעי חיה\"'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_summary(text,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7162c24-8dbe-4d4d-b218-e354e509a494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    טקסט: 1\\n    שאלה: 2\\n    הנחיות: נא לצטט באופן ישיר מהטקסט כחלק מהתשובה. התשובה צריכה להתייחס במדויק לפרטים המופיעים בטקסט.\\n    תשובה:\\n    '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_prompt(\"1\",\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da59ccb-ee3b-465d-aeae-9b8aece6fcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
